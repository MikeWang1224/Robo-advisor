# -*- coding: utf-8 -*-
"""
å¤šå…¬å¸æ–°èæŠ“å–ç¨‹å¼ï¼ˆå°ç©é›» + é´»æµ· + è¯é›»ï¼‰
ç‰ˆæœ¬ï¼šv7-huggingfaceï¼ˆembedding ç‰ˆ / GitHub Secret ç›¸å®¹ï¼‰
---------------------- --------------------------
âœ” Firestore åªç”¨æ—¥æœŸç•¶ ID
âœ” å„²å­˜æ–°è title + content + æ¼²è·Œ + embedding
âœ” Hugging Face å…è²» Embedding API
âœ” è‹¥ embedding å¤±æ•—ï¼Œè‡ªå‹•å­˜ []
âœ” æ–°å¢æ–°èæ™‚é–“è§£æï¼ŒåªæŠ“ 36 å°æ™‚å…§æ–°è
""" 
 
import os
import time
import json
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
import firebase_admin
from firebase_admin import credentials, firestore
import yfinance as yf

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# ---------------------- è¨­å®š ---------------------- #
HEADERS = {
    'User-Agent': 'Mozilla/5.0'
}

HF_API_URL = "https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2"
HF_TOKEN = os.environ.get("HF_TOKEN")

if not HF_TOKEN:
    raise ValueError("âš ï¸ æ‰¾ä¸åˆ° HF_TOKENï¼Œè«‹åœ¨ GitHub Secrets è¨­å®šï¼")

HF_HEADERS = {
    "Authorization": f"Bearer {HF_TOKEN}"
}

# Firestore åˆå§‹åŒ–
key_dict = json.loads(os.environ["NEWS"])
cred = credentials.Certificate(key_dict)
firebase_admin.initialize_app(cred)
db = firestore.client()

ticker_map = {
    "å°ç©é›»": "2330.TW",
    "é´»æµ·": "2317.TW",
    "è¯é›»": "2303.TW"
}

# ---------------------- æ–°å¢ï¼šæ™‚é–“éæ¿¾ ---------------------- #
def is_recent(published_time, hours=36):
    """åˆ¤æ–·æ–°èæ˜¯å¦åœ¨æœ€è¿‘å¹¾å°æ™‚å…§"""
    now = datetime.now().astimezone()
    return (now - published_time) <= timedelta(hours=hours)

# ---------------------- æŠ“è‚¡åƒ¹æ¼²è·Œ ---------------------- #
def fetch_stock_change(stock_name):
    ticker = ticker_map.get(stock_name)
    if not ticker:
        return "ç„¡è³‡æ–™"
    try:
        df = yf.Ticker(ticker).history(period="2d")
        if len(df) < 2:
            return "ç„¡è³‡æ–™"
        last = df['Close'].iloc[-1]
        prev = df['Close'].iloc[-2]
        diff = last - prev
        pct = diff / prev * 100
        sign = "+" if diff >= 0 else ""
        return f"{sign}{diff:.2f} ({sign}{pct:.2f}%)"
    except:
        return "ç„¡è³‡æ–™"

def add_price_change(news_list, stock_name):
    change = fetch_stock_change(stock_name)
    for n in news_list:
        n["price_change"] = change
    return news_list

# ---------------------- Embeddingï¼ˆHugging Faceï¼‰ ---------------------- #
def generate_embedding(text):
    if not text:
        return []
    try:
        res = requests.post(
            HF_API_URL,
            headers=HF_HEADERS,
            json={"inputs": text[:1000]},  # é¿å…å¤ªé•·
            timeout=20
        )
        data = res.json()
        if isinstance(data, list):
            return data
    except Exception as e:
        print(f"âš ï¸ Embedding å¤±æ•—: {e}")
    return []

# ---------------------- æ–‡ç« å…§æ–‡æŠ“å– ---------------------- #
def fetch_article_content(url, source):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        if source == 'yahoo':
            paragraphs = soup.select('article p') or soup.select('p')
        elif source == 'cnbc':
            paragraphs = soup.select('article p') or soup.select('p')
        else:
            paragraphs = soup.select('div.entry-content p, div.entry-content h2')

        text = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 40])
        return text[:1500] + ('...' if len(text) > 1500 else '')
    except:
        return "ç„¡æ³•å–å¾—æ–°èå…§å®¹"

# ---------------------- TechNews ---------------------- #
def fetch_technews(keyword="å°ç©é›»", limit=30):
    print(f"\nğŸ“¡ TechNewsï¼š{keyword}")
    links, news = [], []
    url = f'https://technews.tw/google-search/?googlekeyword={keyword}'
    try:
        res = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(res.text, 'html.parser')
        for a in soup.find_all('a', href=True):
            href = a['href']
            if href.startswith('https://technews.tw/') and '/tag/' not in href:
                if href not in links:
                    links.append(href)
        links = links[:limit]
    except:
        return []

    for link in links:
        try:
            r = requests.get(link, headers=HEADERS)
            s = BeautifulSoup(r.text, 'html.parser')

            # æ¨™é¡Œ
            title_tag = s.find('h1')
            if not title_tag:
                continue
            title = title_tag.get_text(strip=True)

            # ç™¼å¸ƒæ™‚é–“
            time_tag = s.find("time", class_="entry-date")
            if not time_tag:
                continue
            published_str = time_tag.get_text(strip=True)
            published_dt = datetime.strptime(published_str, "%Y/%m/%d %H:%M").astimezone()
            if not is_recent(published_dt, 36):
                continue  # å¤ªèˆŠçš„æ–°èè·³é

            # å…§å®¹
            content = fetch_article_content(link, 'technews')
            news.append({'title': title, 'content': content, 'published_time': published_dt})
            time.sleep(0.5)
        except:
            continue
    return news

# ---------------------- Yahoo æ–°è ---------------------- #
def fetch_yahoo_news(keyword="å°ç©é›»", limit=30):
    print(f"\nğŸ“¡ Yahooï¼š{keyword}")
    base = "https://tw.news.yahoo.com"
    url = f"{base}/search?p={keyword}&sort=time"
    news_list, seen = [], set()

    try:
        r = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(r.text, 'html.parser')
        links = soup.select('a.js-content-viewer') or soup.select('h3 a')

        for a in links:
            if len(news_list) >= limit:
                break
            title = a.get_text(strip=True)
            if not title or title in seen:
                continue
            seen.add(title)
            href = a.get("href")
            if href and not href.startswith("http"):
                href = base + href

            # æ–‡ç« å…§å®¹èˆ‡æ™‚é–“
            content = fetch_article_content(href, 'yahoo')
            try:
                r2 = requests.get(href, headers=HEADERS)
                s2 = BeautifulSoup(r2.text, 'html.parser')
                time_tag = s2.find("time")
                if not time_tag or not time_tag.has_attr("datetime"):
                    continue
                published_dt = datetime.fromisoformat(time_tag["datetime"].replace("Z", "+00:00")).astimezone()
                if not is_recent(published_dt, 36):
                    continue
            except:
                continue

            news_list.append({'title': title, 'content': content, 'published_time': published_dt})
    except:
        pass

    return news_list

# ---------------------- CNBC ---------------------- #
def fetch_cnbc_news(keyword_list=["TSMC"], limit=20):
    print(f"\nğŸ“¡ CNBCï¼š{'/'.join(keyword_list)}")
    urls = [
        "https://www.cnbc.com/search/?query=" + '+'.join(keyword_list)
    ]
    news, seen = [], set()

    for url in urls:
        try:
            r = requests.get(url, headers=HEADERS)
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = soup.select("article a")

            for a in articles:
                if len(news) >= limit:
                    break
                title = a.get_text(strip=True)
                href = a.get("href")

                if not title or title in seen or not href:
                    continue
                if not any(k.lower() in title.lower() for k in keyword_list):
                    continue

                if not href.startswith("http"):
                    href = "https://www.cnbc.com" + href

                # å…§å®¹
                content = fetch_article_content(href, 'cnbc')

                # æ™‚é–“è§£æ
                try:
                    r2 = requests.get(href, headers=HEADERS)
                    s2 = BeautifulSoup(r2.text, 'html.parser')
                    time_tag = s2.find("time")
                    if not time_tag or not time_tag.has_attr("datetime"):
                        continue
                    published_dt = datetime.fromisoformat(time_tag["datetime"].replace("Z", "+00:00")).astimezone()
                    if not is_recent(published_dt, 36):
                        continue
                except:
                    continue

                seen.add(title)
                news.append({'title': title, 'content': content, 'published_time': published_dt})
        except:
            continue

    return news

# ---------------------- Firestore ---------------------- #
def save_news(news_list, collection):
    doc_id = datetime.now().strftime("%Y%m%d")
    ref = db.collection(collection).document(doc_id)

    data = {}
    for i, n in enumerate(news_list, 1):
        emb = generate_embedding(n.get("content", ""))
        data[f"news_{i}"] = {
            "title": n.get("title", ""),
            "price_change": n.get("price_change", "ç„¡è³‡æ–™"),
            "content": n.get("content", ""),
            "embedding": emb,
            "published_time": n.get("published_time").strftime("%Y-%m-%d %H:%M")
        }

    ref.set(data)
    print(f"âœ… Firestore å„²å­˜å®Œæˆï¼š{collection}/{doc_id}")

# ---------------------- ä¸»ç¨‹å¼ ---------------------- #
if __name__ == "__main__":

    # å°ç©é›»
    tsmc_news = fetch_technews("å°ç©é›»", 30) + fetch_yahoo_news("å°ç©é›»", 30) + fetch_cnbc_news(["TSMC"], 20)
    if tsmc_news:
        tsmc_news = add_price_change(tsmc_news, "å°ç©é›»")
        save_news(tsmc_news, "NEWS")

    # é´»æµ·
    fox_news = fetch_yahoo_news("é´»æµ·", 30)
    if fox_news:
        fox_news = add_price_change(fox_news, "é´»æµ·")
        save_news(fox_news, "NEWS_Foxxcon")

    # è¯é›»
    umc_news = fetch_technews("è¯é›»", 20) + fetch_yahoo_news("è¯é›»", 30) + fetch_cnbc_news(["UMC"], 20)
    if umc_news:
        umc_news = add_price_change(umc_news, "è¯é›»")
        save_news(umc_news, "NEWS_UMC")

    print("\nğŸ‰ å…¨éƒ¨æ–°èæŠ“å–å®Œæˆï¼")
