# -*- coding: utf-8 -*-
"""
Yahoo è²¡ç¶“æ–°èæŠ“å–ï¼ˆå…‰å¯¶ç§‘ï¼‰
â˜… 2025æœ€æ–°ç‰ˆ Yahoo æœå°‹çˆ¬èŸ²ï¼ˆä¸å†ä½¿ç”¨ __NEXT_DATA__ï¼Œæ”¹ä½¿ç”¨ application/jsonï¼‰
â˜… ç¶­æŒä½ åŸæœ¬æ¶æ§‹ï¼šé—œéµå­—åˆ¤æ–· / 36 å°æ™‚å…§ / Firestore å¯«å…¥ / æœ¬åœ° result.txt
"""

import os
import time
import hashlib
import logging
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import json
import re

try:
    from dateutil import parser as dateparser
except:
    dateparser = None

import firebase_admin
from firebase_admin import credentials, firestore

# ---------------- Config ----------------
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
MAX_HOURS = 36

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    )
}

REQUEST_TIMEOUT = 12
MAX_RETRIES = 2
SLEEP_BETWEEN_REQ = 0.4

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")


# ---------------- Firestore init ----------------
if not firebase_admin._apps:
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if not cred_path:
        raise SystemExit("Missing GOOGLE_APPLICATION_CREDENTIALS")
    cred = credentials.Certificate(cred_path)
    firebase_admin.initialize_app(cred)

db = firestore.client()


# ---------------- Helpers ----------------
session = requests.Session()
session.headers.update(HEADERS)

def safe_get(url, params=None):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, params=params, timeout=REQUEST_TIMEOUT)
            r.raise_for_status()
            return r
        except Exception:
            time.sleep(0.5 * attempt)
    return None

def clean_text(s):
    return re.sub(r"\s+", " ", s).strip() if s else ""

def now_utc():
    return datetime.now(timezone.utc)

def parse_datetime_fuzzy(s):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except:
        return None

def is_recent(dt):
    if not dt:
        return False
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return (now_utc() - dt).total_seconds() <= MAX_HOURS * 3600

def contains_keywords(text, keywords):
    t = text.lower()
    return any(k.lower() in t for k in keywords)

def doc_id_from_text(t):
    return hashlib.sha1(t.encode("utf-8")).hexdigest()


# ---------------- Yahoo æœå°‹ï¼ˆ2025 æœ€æ–°ç‰ˆï¼‰ ----------------
def fetch_yahoo_all(keywords=None, pages=5):
    if keywords is None:
        keywords = KEYWORDS

    base = "https://tw.search.yahoo.com/search"
    results = []
    seen = set()

    logging.info("ğŸ“¡ Yahoo æ–°ç‰ˆæœå°‹é–‹å§‹â€¦")

    for kw in keywords:
        for pg in range(1, pages + 1):
            params = {
                "p": kw,
                "b": (pg - 1) * 10 + 1,
                "pz": 10,
            }

            r = safe_get(base, params=params)
            if not r:
                continue

            soup = BeautifulSoup(r.text, "html.parser")

            # Yahoo æ–°ç‰ˆæœå°‹ JSONï¼Œä¸å†ä½¿ç”¨ __NEXT_DATA__
            script_tag = soup.find("script", {"type": "application/json"})
            if not script_tag:
                logging.warning("æ‰¾ä¸åˆ° application/jsonï¼ŒYahoo æœå°‹é å¯èƒ½æ›´æ–°ï¼Ÿ")
                continue

            try:
                data = json.loads(script_tag.string)
            except:
                logging.warning("JSON æ ¼å¼éŒ¯èª¤ï¼Œç•¥é")
                continue

            modules = (
                data.get("props", {})
                    .get("pageProps", {})
                    .get("layout", {})
                    .get("main", {})
                    .get("modules", [])
            )

            for m in modules:
                if m.get("name") != "web":
                    continue

                for item in m.get("data", []):
                    url = item.get("url")
                    title = item.get("title")
                    src = item.get("source")
                    ts = item.get("date")

                    if not url or url in seen:
                        continue
                    seen.add(url)

                    # é—œéµå­—éæ¿¾
                    if not contains_keywords(title or "", ["å…‰å¯¶", "å…‰å¯¶ç§‘", "2301"]):
                        continue

                    # æ™‚é–“
                    if ts:
                        dt = datetime.fromtimestamp(ts / 1000, tz=timezone.utc)
                    else:
                        dt = None

                    if not dt or not is_recent(dt):
                        continue

                    # æŠ“å…§æ–‡
                    time.sleep(SLEEP_BETWEEN_REQ)
                    r2 = safe_get(url)
                    if not r2:
                        continue

                    s2 = BeautifulSoup(r2.text, "html.parser")
                    content = ""

                    for sel in [
                        "article p",
                        "div.caas-body p",
                        "div.caas-content p",
                        "div[class*='caas'] p"
                    ]:
                        paras = s2.select(sel)
                        if paras:
                            text = "\n".join([clean_text(p.get_text()) for p in paras])
                            if len(text) > 40:
                                content = text
                                break

                    if len(content) < 30:
                        continue

                    results.append({
                        "title": title,
                        "content": content[:2500],
                        "time": dt.isoformat(),
                        "source": src or "Yahoo",
                    })

            logging.info(f"é—œéµå­— {kw} ç¬¬ {pg} é å®Œæˆï¼Œç›®å‰ç´¯ç© {len(results)} å‰‡")

    logging.info(f"ğŸ“Œ Yahoo æ–°ç‰ˆæœå°‹å®Œæˆï¼Œå…±æŠ“åˆ° {len(results)} å‰‡å…‰å¯¶ç§‘æ–°è")
    return results


# ---------------- Firestore ----------------
def save_to_firestore(article_list):
    if not article_list:
        logging.info("Firestore ç„¡éœ€å¯«å…¥ï¼ˆ0 ç¯‡ï¼‰")
        return

    date_key = datetime.now().strftime("%Y%m%d")
    doc = db.collection(COLL_NAME).document(date_key).collection("articles")

    added = 0
    for art in article_list:
        doc_id = doc_id_from_text(art["title"] + art["time"])
        ref = doc.document(doc_id)
        if ref.get().exists:
            continue
        ref.set(art)
        added += 1

    logging.info(f"Firestore æ–°å¢ {added} ç¯‡")


# ---------------- Local TXT ----------------
def save_to_local(article_list, filename="result.txt"):
    with open(filename, "w", encoding="utf-8") as f:

        if not article_list:
            f.write("ä»Šæ—¥æ²’æœ‰ä»»ä½•å…‰å¯¶ç§‘æ–°èã€‚\n")
            logging.info("result.txt å·²å¯«å…¥ï¼ˆç„¡æ–°èï¼‰")
            return

        for art in article_list:
            f.write(f"[{art['time']}] {art['title']}\n")
            f.write(art["content"] + "\n")
            f.write(f"ä¾†æºï¼š{art['source']}\n")
            f.write("-" * 60 + "\n")

    logging.info("result.txt å·²å¯«å…¥ï¼ˆæœ‰å…§å®¹ï¼‰")


# ---------------- Main ----------------
def main():
    logging.info("é–‹å§‹æŠ“å– Yahoo å…‰å¯¶ç§‘æ–°èï¼ˆæ–°ç‰ˆï¼‰")

    all_news = fetch_yahoo_all()

    save_to_firestore(all_news)
    save_to_local(all_news)

    logging.info("æŠ“å–å®Œæˆã€‚")

if __name__ == "__main__":
    main()
