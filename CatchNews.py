# -*- coding: utf-8 -*-
"""
å…‰å¯¶ç§‘æ–°èæŠ“å–ï¼ˆYahooï¼‰
âœ” åªæŠ“å…‰å¯¶ç§‘
âœ” æŠ“æ–°èå…¨æ–‡
âœ” æ™‚é–“éæ¿¾ï¼š36 å°æ™‚å…§
âœ” å¯«å…¥ Firestoreï¼ˆä¸å­˜è‚¡åƒ¹ï¼‰
"""

import os
import time
import requests
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore

# -----------------------------
# Firestore åˆå§‹åŒ–
# -----------------------------
if not firebase_admin._apps:
    cred = credentials.Certificate(os.environ["GOOGLE_APPLICATION_CREDENTIALS"])
    firebase_admin.initialize_app(cred)

db = firestore.client()
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
MAX_HOURS = 36
HEADERS = {"User-Agent": "Mozilla/5.0"}

# -----------------------------
# æ™‚é–“éæ¿¾
# -----------------------------
def is_recent(dt):
    return (datetime.now(timezone.utc) - dt).total_seconds() <= MAX_HOURS * 3600

# -----------------------------
# Yahoo æŠ“å–
# -----------------------------
def fetch_yahoo(keyword="å…‰å¯¶ç§‘", limit=30):
    print(f"\nğŸ“¡ Yahooï¼š{keyword}")
    base = "https://tw.news.yahoo.com"
    url = f"{base}/search?p={keyword}&sort=time"

    news_list, seen = [], set()

    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        # Yahoo æœå°‹çµæœ
        links = soup.select('a.js-content-viewer') or soup.select('h3 a')

        for a in links:
            if len(news_list) >= limit:
                break

            title = a.get_text(strip=True)
            if not title or title in seen:
                continue
            seen.add(title)

            href = a.get("href")
            if href and not href.startswith("http"):
                href = base + href

            # -----------------------------
            # æŠ“å–å…§æ–‡ + æ™‚é–“
            # -----------------------------
            try:
                r2 = requests.get(href, headers=HEADERS, timeout=10)
                s2 = BeautifulSoup(r2.text, 'html.parser')

                # ---- æŠ“å…¨æ–‡ï¼ˆé¿å… 403 ç°¡åŒ–ç‰ˆæœ¬ï¼‰----
                paras = s2.select("article p") or s2.select("p")
                content = "\n".join(
                    p.get_text(strip=True)
                    for p in paras
                    if len(p.get_text(strip=True)) > 40
                )[:1500]

                # ---- æŠ“æ™‚é–“ ----
                time_tag = s2.find("time")
                if not time_tag or not time_tag.has_attr("datetime"):
                    continue

                published_dt = datetime.fromisoformat(
                    time_tag["datetime"].replace("Z", "+00:00")
                )

                if not is_recent(published_dt):
                    continue

                news_list.append({
                    "title": title,
                    "content": content,
                    "time": published_dt.strftime("%Y-%m-%d %H:%M"),
                    "source": "Yahoo"
                })

                time.sleep(0.3)

            except Exception:
                continue

    except Exception:
        pass

    return news_list

# -----------------------------
# Firestore å¯«å…¥
# -----------------------------
def save_news(news_list):
    if not news_list:
        print("âš ï¸ æ²’æœ‰æ–°èå¯å¯«å…¥")
        return

    today = datetime.now().strftime("%Y%m%d")
    ref = db.collection(COLL_NAME).document(today)

    data = {}
    for i, n in enumerate(news_list, 1):
        data[f"news_{i}"] = n

    ref.set(data)
    print(f"ğŸ”¥ Firestore å·²å¯«å…¥ â†’ {COLL_NAME}/{today}")
    print(f"ğŸ“¦ å…± {len(news_list)} å‰‡æ–°è")

# -----------------------------
# ä¸»ç¨‹å¼
# -----------------------------
if __name__ == "__main__":
    all_news = fetch_yahoo("å…‰å¯¶ç§‘", 30)
    save_news(all_news)
    print("\nğŸ‰ Yahoo æ–°èæŠ“å–å®Œæˆï¼")
