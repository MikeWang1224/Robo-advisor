# -*- coding: utf-8 -*-
"""
Yahoo è²¡ç¶“æ–°èæŠ“å–ï¼ˆå…‰å¯¶ç§‘ï¼‰
æ”¹ç‚ºå¯æ­£å¸¸æŠ“åˆ°æ–°èçš„æ–°ç‰ˆçˆ¬èŸ²ï¼š
â†’ Yahoo æœå°‹é ç¾åœ¨æ”¹æˆ Reactï¼Œæ–°èåˆ—è¡¨å­˜æ–¼ __NEXT_DATA__ JSON
â†’ ç”¨ requests + JSON è§£æå³å¯ç©©å®šå–å¾—çµæœ
"""

import os
import time
import hashlib
import logging
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import json
import re

try:
    from dateutil import parser as dateparser
except:
    dateparser = None

import firebase_admin
from firebase_admin import credentials, firestore

# ---------------- Config ----------------
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
MAX_HOURS = 36

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    )
}

REQUEST_TIMEOUT = 12
MAX_RETRIES = 2
SLEEP_BETWEEN_REQ = 0.4

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")


# ---------------- Firestore init ----------------
if not firebase_admin._apps:
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if not cred_path:
        raise SystemExit("Missing GOOGLE_APPLICATION_CREDENTIALS")
    cred = credentials.Certificate(cred_path)
    firebase_admin.initialize_app(cred)

db = firestore.client()


# ---------------- Helpers ----------------
session = requests.Session()
session.headers.update(HEADERS)

def safe_get(url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, timeout=REQUEST_TIMEOUT)
            r.raise_for_status()
            return r
        except Exception:
            time.sleep(0.5 * attempt)
    return None

def clean_text(s):
    return re.sub(r"\s+", " ", s).strip() if s else ""

def now_utc():
    return datetime.now(timezone.utc)

def parse_datetime_fuzzy(s):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except:
        return None

def is_recent(dt):
    if not dt:
        return False
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return (now_utc() - dt).total_seconds() <= MAX_HOURS * 3600

def contains_keywords(text, keywords):
    t = text.lower()
    return any(k.lower() in t for k in keywords)

def doc_id_from_text(t):
    return hashlib.sha1(t.encode("utf-8")).hexdigest()


# ---------------- Yahoo æ–°ç‰ˆçˆ¬èŸ²ï¼ˆå¯æ­£å¸¸æŠ“æ–°èï¼‰ ----------------
def fetch_yahoo_all(keywords=None, pages=5):
    if keywords is None:
        keywords = KEYWORDS

    base = "https://tw.news.yahoo.com"
    results = []
    seen_url = set()

    logging.info("ğŸ“¡ Yahoo æ–°ç‰ˆæœå°‹é–‹å§‹â€¦")

    for kw in keywords:
        for page in range(1, pages + 1):
            b = (page - 1) * 10 + 1
            url = f"{base}/search?p={kw}&sort=time&b={b}"

            r = safe_get(url)
            if not r:
                continue

            soup = BeautifulSoup(r.text, "html.parser")

            # è§£æ __NEXT_DATA__ï¼ˆæ•´å€‹æ–°èåˆ—è¡¨éƒ½åœ¨é€™è£¡ï¼‰
            script_tag = soup.find("script", id="__NEXT_DATA__", type="application/json")
            if not script_tag:
                logging.warning("æ‰¾ä¸åˆ° __NEXT_DATA__ï¼ŒYahoo æœå°‹é å·²æ”¹ç‰ˆï¼Ÿ")
                continue

            try:
                data = json.loads(script_tag.string)
                items = data["props"]["pageProps"]["initialState"]["search"]["news"]["items"]
            except:
                logging.warning("Yahoo æœå°‹ JSON çµæ§‹ä¸ç¬¦")
                continue

            # è™•ç†æ¯ä¸€å‰‡æ–°è
            for item in items:
                link = item.get("link")
                title = clean_text(item.get("title") or "")
                pub = item.get("pubDate")

                if not link or link in seen_url:
                    continue
                seen_url.add(link)

                # é—œéµå­—éæ¿¾ï¼ˆå…‰å¯¶ï¼‰
                if not contains_keywords(title, ["å…‰å¯¶", "å…‰å¯¶ç§‘", "2301"]):
                    continue

                dt = parse_datetime_fuzzy(pub)
                if not dt or not is_recent(dt):
                    continue

                # æŠ“å…§æ–‡
                time.sleep(SLEEP_BETWEEN_REQ)
                r2 = safe_get(link)
                if not r2:
                    continue

                s2 = BeautifulSoup(r2.text, "html.parser")

                content = ""
                for sel in [
                    "article p",
                    "div.caas-body p",
                    "div.caas-content p",
                    "div[class*='caas'] p"
                ]:
                    paras = s2.select(sel)
                    if paras:
                        text = "\n".join([clean_text(p.get_text()) for p in paras])
                        if len(text) > 40:
                            content = text
                            break

                if len(content) < 30:
                    continue

                results.append({
                    "title": title,
                    "content": content[:2500],
                    "time": dt.isoformat(),
                    "source": "Yahoo"
                })

    logging.info(f"ğŸ“Œ Yahoo æ–°ç‰ˆæœå°‹å®Œæˆï¼Œå…±æŠ“åˆ° {len(results)} å‰‡å…‰å¯¶ç§‘æ–°è")
    return results


# ---------------- Firestore ----------------
def save_to_firestore(article_list):
    if not article_list:
        logging.info("Firestore ç„¡éœ€å¯«å…¥ï¼ˆ0 ç¯‡ï¼‰")
        return

    date_key = datetime.now().strftime("%Y%m%d")
    doc = db.collection(COLL_NAME).document(date_key).collection("articles")

    added = 0
    for art in article_list:
        doc_id = doc_id_from_text(art["title"] + art["time"])
        ref = doc.document(doc_id)
        if ref.get().exists:
            continue
        ref.set(art)
        added += 1

    logging.info(f"Firestore æ–°å¢ {added} ç¯‡")


# ---------------- Local TXT ----------------
def save_to_local(article_list, filename="result.txt"):
    with open(filename, "w", encoding="utf-8") as f:

        if not article_list:
            f.write("ä»Šæ—¥æ²’æœ‰ä»»ä½•å…‰å¯¶ç§‘æ–°èã€‚\n")
            logging.info("result.txt å·²å¯«å…¥ï¼ˆç„¡æ–°èï¼‰")
            return

        for art in article_list:
            f.write(f"[{art['time']}] {art['title']}\n")
            f.write(art["content"] + "\n")
            f.write(f"ä¾†æºï¼š{art['source']}\n")
            f.write("-" * 60 + "\n")

    logging.info("result.txt å·²å¯«å…¥ï¼ˆæœ‰å…§å®¹ï¼‰")


# ---------------- Main ----------------
def main():
    logging.info("é–‹å§‹æŠ“å– Yahoo å…‰å¯¶ç§‘æ–°èï¼ˆæ–°ç‰ˆï¼‰")

    all_news = fetch_yahoo_all()      # å·²æ›´æ–°ç‚ºæ–°ç‰ˆ Yahoo çˆ¬èŸ²

    save_to_firestore(all_news)
    save_to_local(all_news)

    logging.info("æŠ“å–å®Œæˆã€‚")

if __name__ == "__main__":
    main()
