# -*- coding: utf-8 -*-
"""
å…‰å¯¶ç§‘æ–°èæŠ“å–ï¼ˆTechNews + Yahoo + CNBCï¼‰ - ä¿®æ­£ç‰ˆ
âœ” åªæŠ“å…‰å¯¶ç§‘
âœ” æŠ“æ–°èå…¨æ–‡
âœ” æ™‚é–“éæ¿¾ï¼š36 å°æ™‚å…§
âœ” å¯«å…¥ Firestoreï¼Œä¸å­˜è‚¡åƒ¹
âœ” æ›´ç©©å¥çš„ selectorã€é‡è©¦ã€éŒ¯èª¤è™•ç†ã€å»é‡
"""
import os
import time
import json
import re
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import requests
from bs4 import BeautifulSoup

# Optional: better date parsing if python-dateutil is installed
try:
    from dateutil import parser as dateparser
except Exception:
    dateparser = None

# Firestore imports
import firebase_admin
from firebase_admin import credentials, firestore

# -----------------------------
# è¨­å®š
# -----------------------------
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301", "Lite-On", "LiteOn", "Lite On"]
MAX_HOURS = 36
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/117.0.0.0 Safari/537.36"
}
REQUEST_TIMEOUT = 15  # seconds
MAX_RETRIES = 3
SLEEP_BETWEEN_REQ = 0.4

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

# -----------------------------
# Firestore åˆå§‹åŒ–
# -----------------------------
if not firebase_admin._apps:
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if not cred_path:
        logging.error("è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ GOOGLE_APPLICATION_CREDENTIALS æŒ‡å‘ä½ çš„ Firebase key JSON æª”æ¡ˆ")
        raise SystemExit("Missing GOOGLE_APPLICATION_CREDENTIALS")
    cred = credentials.Certificate(cred_path)
    firebase_admin.initialize_app(cred)

db = firestore.client()

# -----------------------------
# å·¥å…·å‡½å¼
# -----------------------------
def now():
    return datetime.now(timezone.utc)

def is_recent(dt: datetime) -> bool:
    """åˆ¤æ–·æ˜¯å¦åœ¨ MAX_HOURS å…§ã€‚æ¥å— timezone-aware dt æˆ– naive (è¦–ç‚ºæœ¬åœ°æ™‚é–“)ã€‚"""
    if dt.tzinfo is None:
        # assume local -> convert to UTC using system local offset
        dt = dt.replace(tzinfo=timezone.utc)
    return (now() - dt).total_seconds() <= MAX_HOURS * 3600

def parse_datetime(dt_str: str) -> datetime:
    """å˜—è©¦è§£ææ™‚é–“å­—ä¸²ç‚º datetimeï¼ˆUTC-awareï¼‰ã€‚è‹¥è§£æå¤±æ•—æœƒæ‹‹ä¾‹å¤–ã€‚"""
    if not dt_str or not isinstance(dt_str, str):
        raise ValueError("ç©ºçš„æ™‚é–“å­—ä¸²")
    dt_str = dt_str.strip()
    # try dateutil if available
    if dateparser:
        try:
            parsed = dateparser.parse(dt_str)
            if parsed is None:
                raise ValueError("dateutil ç„¡æ³•è§£æ")
            # make timezone-aware: if naive, assume UTC (many news sites use ISO Z)
            if parsed.tzinfo is None:
                parsed = parsed.replace(tzinfo=timezone.utc)
            return parsed.astimezone(timezone.utc)
        except Exception:
            pass
    # fallback common patterns
    # ISO-like
    iso_try = re.sub(r'(\.\d+)?Z$', '+00:00', dt_str)
    try:
        parsed = datetime.fromisoformat(iso_try)
        if parsed.tzinfo is None:
            parsed = parsed.replace(tzinfo=timezone.utc)
        return parsed.astimezone(timezone.utc)
    except Exception:
        pass
    # common formats
    for fmt in ("%Y/%m/%d %H:%M", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M"):
        try:
            parsed = datetime.strptime(dt_str, fmt)
            parsed = parsed.replace(tzinfo=timezone.utc)
            return parsed
        except Exception:
            continue
    raise ValueError(f"ç„¡æ³•è§£ææ™‚é–“: {dt_str}")

# requests session with retries
session = requests.Session()
adapter = requests.adapters.HTTPAdapter(max_retries=3)
session.mount("http://", adapter)
session.mount("https://", adapter)
session.headers.update(HEADERS)

def safe_get(url: str, headers: dict = None, timeout=REQUEST_TIMEOUT):
    headers = headers or {}
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r
        except requests.RequestException as e:
            logging.warning(f"GET {url} å¤±æ•— (attempt {attempt}): {e}")
            if attempt < MAX_RETRIES:
                time.sleep(0.5 * attempt)
            else:
                return None

def clean_text(s: str) -> str:
    return re.sub(r'\s+', ' ', s).strip()

# -----------------------------
# TechNews
# -----------------------------
def fetch_technews(keyword="å…‰å¯¶ç§‘", limit=30) -> List[Dict]:
    logging.info(f"ğŸ“¡ TechNewsï¼š{keyword}")
    links = []
    results = []
    # æ”¹ç‚ºå®˜æ–¹æœå°‹ page(s)
    search_url = f"https://technews.tw/page/1/?s={requests.utils.requote_uri(keyword)}"
    r = safe_get(search_url)
    if not r:
        logging.warning("TechNews æœå°‹é æŠ“å–å¤±æ•—")
        return results
    s = BeautifulSoup(r.text, "html.parser")
    # WordPress å¸¸ç”¨æ–‡ç« å¡ç‰‡ selector
    cards = s.select("article.post a") or s.select("h2.entry-title a") or s.select(".entry-title a")
    for a in cards:
        href = a.get("href")
        if href and href.startswith("https://technews.tw/"):
            links.append(href)
    # å»é‡ä¸¦é™åˆ¶æ•¸é‡
    links = list(dict.fromkeys(links))[:limit]
    for link in links:
        time.sleep(SLEEP_BETWEEN_REQ)
        r2 = safe_get(link)
        if not r2:
            continue
        s2 = BeautifulSoup(r2.text, "html.parser")
        title_tag = s2.find('h1') or s2.select_one(".entry-title") or s2.select_one("h1.entry-title")
        if not title_tag:
            continue
        title = clean_text(title_tag.get_text())
        # time tag: TechNews ç”¨ <time class="entry-date">YYYY/MM/DD HH:MM</time>
        time_tag = s2.find("time", class_="entry-date")
        published_dt = None
        if time_tag:
            try:
                published_dt = parse_datetime(time_tag.get_text(strip=True))
            except Exception:
                pass
        # fallback: meta[property="article:published_time"]
        if not published_dt:
            meta = s2.find("meta", {"property": "article:published_time"}) or s2.find("meta", {"name": "pubdate"})
            if meta and meta.has_attr("content"):
                try:
                    published_dt = parse_datetime(meta["content"])
                except Exception:
                    pass
        if not published_dt:
            # ç„¡æ™‚é–“è³‡æ–™å‰‡è·³éï¼ˆé¿å…æŠ“åˆ°ç„¡é—œå…§å®¹ï¼‰
            continue
        if not is_recent(published_dt):
            continue
        paras = s2.select("article p") or s2.select("p")
        content = "\n".join([clean_text(p.get_text()) for p in paras if len(clean_text(p.get_text())) > 40])
        if not content:
            continue
        results.append({
            "title": title,
            "content": content[:1500],
            "time": published_dt.strftime("%Y-%m-%d %H:%M %Z"),
            "source": "TechNews",
            "url": link
        })
    logging.info(f"TechNews æŠ“åˆ° {len(results)} ç­†ç¬¦åˆçš„æ–°è")
    return results

# -----------------------------
# Yahoo
# -----------------------------
def fetch_yahoo(keyword="å…‰å¯¶ç§‘", limit=30) -> List[Dict]:
    logging.info(f"ğŸ“¡ Yahooï¼š{keyword}")
    base = "https://tw.news.yahoo.com"
    # ç”¨ search?p=keyword&sort=time
    search_url = f"{base}/search?p={requests.utils.requote_uri(keyword)}&sort=time"
    news_list = []
    seen_titles = set()
    r = safe_get(search_url)
    if not r:
        logging.warning("Yahoo æœå°‹é æŠ“å–å¤±æ•—")
        return news_list
    s = BeautifulSoup(r.text, "html.parser")
    # æœå°‹çµæœçš„é€£çµå¯èƒ½åœ¨å¤šç¨®ä½ç½®ï¼Œå˜—è©¦å¹¾ç¨® selector
    link_selectors = [
        'a.js-content-viewer',  # èˆŠç‰ˆ
        'h3 a',                 # å¸¸è¦‹
        'a[href*="/news/"]',    # ç›´æ¥éæ¿¾ news è·¯å¾‘
        'a[href*="/articles/"]'
    ]
    candidates = []
    for sel in link_selectors:
        candidates += s.select(sel)
    # ä»¥ href éæ¿¾ï¼Œä¸”ç¢ºä¿é€£åˆ° tw.news.yahoo.com æˆ–å¤–éƒ¨ä¾†æºçš„æ–°èé 
    links = []
    for a in candidates:
        href = a.get("href") or a.get("data-href")
        if not href:
            continue
        # å¯èƒ½æ˜¯ç›¸å°è·¯å¾‘
        if href.startswith("/"):
            href = base + href
        # exclude ad/tracking
        if 'yahoo.com/amp' in href or 'video' in href:
            # still allow amp if it's article
            pass
        links.append((href, clean_text(a.get_text() or "")))
    # å»é‡é †åºä¿æŒ
    seen = set()
    filtered_links = []
    for href, title_text in links:
        if href in seen:
            continue
        seen.add(href)
        filtered_links.append((href, title_text))
        if len(filtered_links) >= limit * 2:
            break
    for href, title_text in filtered_links:
        if len(news_list) >= limit:
            break
        time.sleep(SLEEP_BETWEEN_REQ)
        r2 = safe_get(href)
        if not r2:
            continue
        s2 = BeautifulSoup(r2.text, "html.parser")
        # è©¦è‘—æŠ“ title èˆ‡æ™‚é–“
        title = title_text or (s2.find("h1") and clean_text(s2.find("h1").get_text())) or ""
        if not title:
            # fallback meta
            meta_title = s2.find("meta", {"property": "og:title"}) or s2.find("meta", {"name": "title"})
            if meta_title and meta_title.get("content"):
                title = clean_text(meta_title["content"])
        if not title or title in seen_titles:
            continue
        # time: <time datetime="..."> æˆ– meta property article
        published_dt = None
        time_tag = s2.find("time")
        if time_tag and time_tag.has_attr("datetime"):
            try:
                published_dt = parse_datetime(time_tag["datetime"])
            except Exception:
                # sometimes it's inner text
                try:
                    published_dt = parse_datetime(time_tag.get_text(strip=True))
                except Exception:
                    published_dt = None
        if not published_dt:
            meta = s2.find("meta", {"property": "article:published_time"}) or s2.find("meta", {"name": "ptime"})
            if meta and meta.has_attr("content"):
                try:
                    published_dt = parse_datetime(meta["content"])
                except Exception:
                    published_dt = None
        if not published_dt:
            # å¦‚æœæ²’æœ‰æ™‚é–“ï¼Œè·³éä»¥å…è¶…æ™‚æˆ–æŠ“åˆ°ä¸æ˜¯æ–°èçš„é é¢
            continue
        if not is_recent(published_dt):
            continue
        paras = s2.select("article p") or s2.select("div[class*='article'] p') or s2.select("p")
        content = "\n".join([clean_text(p.get_text()) for p in paras if len(clean_text(p.get_text())) > 40])
        if not content:
            continue
        # é—œéµå­—æª¢æŸ¥ï¼šç¢ºä¿å…§æ–‡æˆ–æ¨™é¡Œæœ‰é—œéµå­—ï¼ˆé¿å…æŠ“åˆ°ç„¡é—œé é¢ï¼‰
        if not any(k.lower() in (title + content).lower() for k in KEYWORDS):
            continue
        news_list.append({
            "title": title,
            "content": content[:1500],
            "time": published_dt.strftime("%Y-%m-%d %H:%M %Z"),
            "source": "Yahoo",
            "url": href
        })
        seen_titles.add(title)
    logging.info(f"Yahoo æŠ“åˆ° {len(news_list)} ç­†ç¬¦åˆçš„æ–°è")
    return news_list

# -----------------------------
# CNBCï¼ˆä¿ç•™ä½†å®¹éŒ¯ï¼‰
# -----------------------------
def fetch_cnbc(keyword_list=["Lite-On"], limit=20) -> List[Dict]:
    logging.info(f"ğŸ“¡ CNBCï¼š{'/'.join(keyword_list)}")
    base_search = "https://www.cnbc.com/search/?query=" + '+'.join(requests.utils.requote_uri(k) for k in keyword_list)
    results = []
    r = safe_get(base_search)
    if not r:
        logging.warning("CNBC æœå°‹é æŠ“å–å¤±æ•—")
        return results
    s = BeautifulSoup(r.text, "html.parser")
    articles = s.select("article a") or s.select(".SearchResult-card a") or s.select("a.Card-title")
    seen = set()
    for a in articles:
        if len(results) >= limit:
            break
        href = a.get("href")
        title = clean_text(a.get_text() or "")
        if not href or not title:
            continue
        # è·³éé‡è¤‡
        if title in seen:
            continue
        # ç¢ºèªæ¨™é¡Œå«é—œéµå­—
        if not any(k.lower() in title.lower() for k in keyword_list):
            # ä¹Ÿå¯å˜—è©¦åˆ°å…§æ–‡æª¢æŸ¥ï¼Œä½†å…ˆéæ¿¾ä¸€è¼ª
            pass
        # å®Œæ•´é€£çµ
        if not href.startswith("http"):
            href = "https://www.cnbc.com" + href
        time.sleep(SLEEP_BETWEEN_REQ)
        r2 = safe_get(href)
        if not r2:
            continue
        s2 = BeautifulSoup(r2.text, "html.parser")
        # è©¦æŠ“æ™‚é–“
        published_dt = None
        time_tag = s2.find("time")
        if time_tag and time_tag.has_attr("datetime"):
            try:
                published_dt = parse_datetime(time_tag["datetime"])
            except Exception:
                pass
        if not published_dt:
            meta = s2.find("meta", {"property": "article:published_time"})
            if meta and meta.has_attr("content"):
                try:
                    published_dt = parse_datetime(meta["content"])
                except Exception:
                    pass
        if not published_dt:
            continue
        if not is_recent(published_dt):
            continue
        paras = s2.select("p")
        content = "\n".join([clean_text(p.get_text()) for p in paras if len(clean_text(p.get_text())) > 40])
        if not content:
            continue
        # å†æª¢æŸ¥é—œéµå­—
        if not any(k.lower() in (title + content).lower() for k in KEYWORDS):
            continue
        results.append({
            "title": title,
            "content": content[:1500],
            "time": published_dt.strftime("%Y-%m-%d %H:%M %Z"),
            "source": "CNBC",
            "url": href
        })
        seen.add(title)
    logging.info(f"CNBC æŠ“åˆ° {len(results)} ç­†ç¬¦åˆçš„æ–°è")
    return results

# -----------------------------
# Firestore å¯«å…¥
# -----------------------------
def save_news(news_list: List[Dict]):
    if not news_list:
        logging.warning("âš ï¸ æ²’æœ‰æ–°èå¯å¯«å…¥")
        return
    # å»é‡ï¼ˆä»¥ title + source ç‚º keyï¼‰
    unique = {}
    for n in news_list:
        key = (n.get("title","").strip(), n.get("source",""))
        if key not in unique:
            unique[key] = n
    news_items = list(unique.values())
    today = datetime.now().strftime("%Y%m%d")
    ref = db.collection(COLL_NAME).document(today)
    data = {}
    for i, n in enumerate(news_items, 1):
        data[f"news_{i}"] = n
    try:
        ref.set(data)
        logging.info(f"ğŸ”¥ Firestore å·²å¯«å…¥ â†’ {COLL_NAME}/{today}")
        logging.info(f"ğŸ“¦ å…± {len(news_items)} å‰‡æ–°è")
    except Exception as e:
        logging.error(f"Firestore å¯«å…¥å¤±æ•—: {e}")

# -----------------------------
# ä¸»ç¨‹å¼
# -----------------------------
def main():
    all_news = []
    try:
        all_news += fetch_technews("å…‰å¯¶ç§‘", 30)
    except Exception as e:
        logging.exception("TechNews æŠ“å–ç™¼ç”ŸéŒ¯èª¤ï¼š%s", e)
    try:
        all_news += fetch_yahoo("å…‰å¯¶ç§‘", 30)
    except Exception as e:
        logging.exception("Yahoo æŠ“å–ç™¼ç”ŸéŒ¯èª¤ï¼š%s", e)
    try:
        # CNBC é—œéµå­—æ¡å¤šå½¢æ…‹ï¼Œæ¯”å°è¼ƒå¯¬é¬†
        all_news += fetch_cnbc(["Lite-On", "LiteOn", "Lite On"], 20)
    except Exception as e:
        logging.exception("CNBC æŠ“å–ç™¼ç”ŸéŒ¯èª¤ï¼š%s", e)

    # æœ€å¾Œå†å»ä¸€æ¬¡é—œéµå­—éæ¿¾ï¼ˆä¿éšªï¼‰
    filtered = []
    for n in all_news:
        if any(k.lower() in (n.get("title","") + n.get("content","")).lower() for k in KEYWORDS):
            filtered.append(n)
    save_news(filtered)
    logging.info("ğŸ‰ å…¨éƒ¨æ–°èæŠ“å–å®Œæˆï¼")

if __name__ == "__main__":
    main()
