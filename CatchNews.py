# -*- coding: utf-8 -*-
"""
Yahoo è²¡ç¶“æ–°èæŠ“å–ï¼ˆå…‰å¯¶ç§‘ï¼‰
æŠ“æ‰€æœ‰å…‰å¯¶ç§‘æ–°è â†’ å†æŒ‘è²¡å ±/æ³•èªª/å…¬å‘Š
æ™‚é–“ç¯©é¸ï¼š36 å°æ™‚
Firestoreï¼šNEWS_LiteOn / YYYYMMDD / articles
æœ¬åœ°ï¼šresult.txtï¼ˆæ°¸ä¸ç‚ºç©ºï¼‰
"""
import os
import time
import hashlib
import logging
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
import re

try:
    from dateutil import parser as dateparser
except:
    dateparser = None

import firebase_admin
from firebase_admin import credentials, firestore

# ---------------- Config ----------------
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
FIN_KEYWORDS = ["è²¡å ±", "æ³•èªª", "å­£å ±", "å…¬å‘Š"]
MAX_HOURS = 36

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    )
}

REQUEST_TIMEOUT = 12
MAX_RETRIES = 2
SLEEP_BETWEEN_REQ = 0.4

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")


# ---------------- Firestore init ----------------
if not firebase_admin._apps:
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if not cred_path:
        raise SystemExit("Missing GOOGLE_APPLICATION_CREDENTIALS")
    cred = credentials.Certificate(cred_path)
    firebase_admin.initialize_app(cred)
db = firestore.client()


# ---------------- Helpers ----------------
session = requests.Session()
session.headers.update(HEADERS)

def safe_get(url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, timeout=REQUEST_TIMEOUT)
            r.raise_for_status()
            return r
        except Exception:
            time.sleep(0.5 * attempt)
    return None

def clean_text(s):
    return re.sub(r"\s+", " ", s).strip() if s else ""

def now_utc():
    return datetime.now(timezone.utc)

def parse_datetime_fuzzy(s):
    if not s:
        return None
    try:
        dt = dateparser.parse(s)
        if dt and dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except:
        return None

def is_recent(dt):
    if not dt:
        return False
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return (now_utc() - dt).total_seconds() <= MAX_HOURS * 3600

def contains_keywords(text, keywords):
    t = text.lower()
    return any(k.lower() in t for k in keywords)

def doc_id_from_url(url):
    return hashlib.sha1(url.encode("utf-8")).hexdigest()


# ---------------- Yahoo æŠ“å…¨éƒ¨å…‰å¯¶æ–°è ----------------
def fetch_yahoo_all(keywords=None, pages=5):
    if keywords is None:
        keywords = KEYWORDS

    base = "https://tw.news.yahoo.com"
    results = []
    seen = set()

    logging.info("ğŸ“¡ Yahoo æœå°‹ï¼ˆä¸ç¯©è²¡å ±ï¼‰é–‹å§‹â€¦")

    for kw in keywords:
        for page in range(1, pages + 1):
            b = (page - 1) * 10 + 1
            url = f"{base}/search?p={kw}&sort=time&b={b}"

            r = safe_get(url)
            if not r:
                continue

            soup = BeautifulSoup(r.text, "html.parser")
            links = soup.select("a.js-content-viewer, h3 a, a[href*='/news/']")

            for a in links:
                href = a.get("href")
                if not href:
                    continue
                if href.startswith("/"):
                    href = base + href
                if href in seen:
                    continue
                seen.add(href)

                # æŠ“å…§é 
                time.sleep(SLEEP_BETWEEN_REQ)
                r2 = safe_get(href)
                if not r2:
                    continue

                s2 = BeautifulSoup(r2.text, "html.parser")

                # æ¨™é¡Œ
                title = clean_text(s2.find("h1").get_text()) if s2.find("h1") else ""
                if not title:
                    continue

                # å¿…é ˆåŒ…å«å…‰å¯¶é—œéµå­—
                if not contains_keywords(title, ["å…‰å¯¶", "å…‰å¯¶ç§‘", "2301"]):
                    continue

                # æ™‚é–“
                t = s2.find("time")
                dt = None
                if t and t.has_attr("datetime"):
                    dt = parse_datetime_fuzzy(t["datetime"])

                if not dt or not is_recent(dt):
                    continue

                # æŠ“å…§æ–‡ï¼ˆå¼·åŒ– selectorï¼‰
                selectors = [
                    "article p",
                    "div.caas-body p",
                    "div.caas-content p",
                    "div[class*='caas'] p"
                ]
                content = ""
                for sel in selectors:
                    paras = s2.select(sel)
                    if paras:
                        text = "\n".join([clean_text(p.get_text()) for p in paras])
                        if len(text) > 40:
                            content = text
                            break
                if len(content) < 30:
                    continue

                results.append({
                    "title": title,
                    "content": content[:2500],
                    "time": dt.isoformat(),
                    "url": href,
                    "source": "Yahoo"
                })

    logging.info(f"Yahoo æœå°‹å®Œæˆï¼Œå…±æŠ“åˆ° {len(results)} å‰‡å…‰å¯¶ç§‘æ–°èï¼ˆå°šæœªç¯©è²¡å ±ï¼‰")
    return results


# ---------------- éæ¿¾è²¡å ±/æ³•èªªé¡ ----------------
def filter_financial_news(articles):
    fin = []
    for a in articles:
        if contains_keywords(a["title"] + " " + a["content"], FIN_KEYWORDS):
            fin.append(a)
    logging.info(f"ç¶“è²¡å ±ç¯©é¸å¾Œï¼Œå…± {len(fin)} å‰‡")
    return fin


# ---------------- Firestore ----------------
def save_to_firestore(article_list):
    if not article_list:
        logging.info("Firestore ç„¡éœ€å¯«å…¥ï¼ˆ0 ç¯‡ï¼‰")
        return

    date_key = datetime.now().strftime("%Y%m%d")
    doc = db.collection(COLL_NAME).document(date_key).collection("articles")

    added = 0
    for art in article_list:
        uid = doc_id_from_url(art["url"])
        ref = doc.document(uid)
        if ref.get().exists:
            continue
        ref.set(art)
        added += 1

    logging.info(f"Firestore æ–°å¢ {added} ç¯‡")


# ---------------- Local TXT ----------------
def save_to_local(article_list, filename="result.txt"):
    with open(filename, "w", encoding="utf-8") as f:

        if not article_list:
            f.write("ä»Šæ—¥æ²’æœ‰ä»»ä½•ç¬¦åˆï¼ˆè²¡å ±/æ³•èªª/å…¬å‘Šï¼‰çš„å…‰å¯¶ç§‘æ–°èã€‚\n")
            logging.info("result.txt å·²å¯«å…¥ï¼ˆç„¡æ–°èä½†ä¸ç‚ºç©ºï¼‰")
            return

        for art in article_list:
            f.write(f"[{art['time']}] {art['title']}\n")
            f.write(art['content'] + "\n")
            f.write(f"URL: {art['url']}\n")
            f.write("-" * 60 + "\n")

    logging.info("result.txt å·²å¯«å…¥ï¼ˆæœ‰å…§å®¹ï¼‰")


# ---------------- Main ----------------
def main():
    logging.info("é–‹å§‹æŠ“å– Yahoo å…‰å¯¶ç§‘æ–°èï¼ˆå®Œæ•´æ¨¡å¼ï¼‰")

    all_news = fetch_yahoo_all()          # æŠ“æ‰€æœ‰å…‰å¯¶æ–°è
    fin_news = filter_financial_news(all_news)  # ç¯©è²¡å ±/æ³•èªª/å…¬å‘Š

    save_to_firestore(fin_news)
    save_to_local(fin_news)

    logging.info("æŠ“å–å®Œæˆã€‚")

if __name__ == "__main__":
    main()
