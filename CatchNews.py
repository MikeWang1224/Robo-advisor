# -*- coding: utf-8 -*-
"""
å…‰å¯¶ç§‘æ–°èæŠ“å–ï¼ˆYahoo å€‹è‚¡æ–°è + é‰…äº¨ç¶²ï¼‰
åªæŠ“å…‰å¯¶ç§‘ + 36 å°æ™‚å…§æ–°è  
"""

import os
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
import firebase_admin
from firebase_admin import credentials, firestore

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# ----- è¨­å®š -----
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Firestore åˆå§‹åŒ–
cred = credentials.Certificate(os.environ["GOOGLE_APPLICATION_CREDENTIALS"])
firebase_admin.initialize_app(cred)
db = firestore.client()


# ----- æ™‚é–“éæ¿¾ -----
def is_recent(published_time, hours=36):
    now = datetime.now().astimezone()
    return (now - published_time) <= timedelta(hours=hours)


# ----- æŠ“æ–‡ç« å…§å®¹ -----
def fetch_article_content(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        paragraphs = soup.select('article p') or soup.select('p')
        text = "\n".join([p.get_text(strip=True) for p in paragraphs])
        return text[:1500] + ('...' if len(text) > 1500 else '')
    except:
        return "ç„¡æ³•å–å¾—æ–°èå…§å®¹"


# =============================
#  Yahoo å€‹è‚¡æ–°èï¼ˆå¤§é‡æŠ“ï¼‰
# =============================
def fetch_yahoo_news_stock(symbol="2301", limit=50):
    print(f"ğŸ“¡ Yahoo å€‹è‚¡æ–°èï¼š{symbol}")

    url = f"https://tw.stock.yahoo.com/quote/{symbol}/news"
    news_list, seen = [], set()

    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")

        # å€‹è‚¡æ–°èåˆ—è¡¨
        cards = soup.select("ul[data-test-locator='stream-container'] li")

        for card in cards:
            if len(news_list) >= limit:
                break

            a = card.find("a")
            if not a:
                continue

            title = a.get_text(strip=True)
            if not title or title in seen:
                continue
            seen.add(title)

            href = a.get("href")
            if href.startswith("/"):
                href = "https://tw.stock.yahoo.com" + href

            # å†æŠ“æ–‡ç« ï¼Œçœ‹å¯¦éš›ç™¼å¸ƒæ™‚é–“
            try:
                r2 = requests.get(href, headers=HEADERS, timeout=10)
                s2 = BeautifulSoup(r2.text, "html.parser")

                time_tag = s2.find("time")
                if not time_tag or not time_tag.has_attr("datetime"):
                    continue

                published_dt = datetime.fromisoformat(
                    time_tag["datetime"].replace("Z", "+00:00")
                ).astimezone()

                if not is_recent(published_dt):
                    continue

                content = fetch_article_content(href)

                news_list.append({
                    "title": title,
                    "content": content,
                    "published_time": published_dt,
                    "source": "Yahoo"
                })

            except:
                continue

    except Exception as e:
        print("Yahoo å€‹è‚¡æ–°èéŒ¯èª¤ï¼š", e)

    return news_list


# =============================
#  é‰…äº¨ç¶²ï¼ˆå« fallback é—œéµå­—ï¼‰
# =============================
def fetch_cnyes_news(keyword="å…‰å¯¶ç§‘", limit=30):
    print(f"ğŸ“¡ é‰…äº¨ç¶²ï¼š{keyword}")

    keywords = [keyword, "å…‰å¯¶", "2301"]  # fallback

    news_list = []

    for kw in keywords:
        try:
            url = f"https://api.cnyes.com/media/api/v1/search/list?keyword={kw}&limit=30"
            r = requests.get(url, headers=HEADERS, timeout=10)
            data = r.json()

            items = data.get("items", {}).get("data", [])

            if not items:
                print(f"âš  é‰…äº¨æœå°‹ã€Œ{kw}ã€ç„¡è³‡æ–™")
                continue

            for item in items:
                if len(news_list) >= limit:
                    break

                title = item.get("title", "")
                if not title:
                    continue

                timestamp = item.get("publishAt", 0)
                if timestamp == 0:
                    continue

                published_dt = datetime.fromtimestamp(timestamp).astimezone()

                if not is_recent(published_dt):
                    continue

                article_url = f"https://news.cnyes.com/news/id/{item.get('newsId')}?exp=a"
                content = fetch_article_content(article_url)

                news_list.append({
                    "title": title,
                    "content": content,
                    "published_time": published_dt,
                    "source": "é‰…äº¨ç¶²"
                })

            if news_list:
                break

        except Exception as e:
            print("é‰…äº¨ç¶²éŒ¯èª¤ï¼š", e)

    return news_list


# =============================
# Firestore å„²å­˜
# =============================
def save_news(news_list):
    doc_id = datetime.now().strftime("%Y%m%d")
    ref = db.collection("NEWS_LiteOn").document(doc_id)

    data = {}
    for i, n in enumerate(news_list, 1):
        data[f"news_{i}"] = {
            "title": n["title"],
            "content": n["content"],
            "published_time": n["published_time"].strftime("%Y-%m-%d %H:%M"),
            "source": n["source"]
        }

    ref.set(data)
    print(f"âœ… Firestore å„²å­˜å®Œæˆï¼šNEWS_LiteOn/{doc_id}")


# =============================
# ä¸»ç¨‹å¼
# =============================
if __name__ == "__main__":
    yahoo_news = fetch_yahoo_news_stock("2301", 50)
    cnyes_news = fetch_cnyes_news("å…‰å¯¶ç§‘", 30)

    all_news = yahoo_news + cnyes_news

    if all_news:
        save_news(all_news)

    print("\nğŸ‰ å…‰å¯¶ç§‘ï¼ˆ2301ï¼‰æ–°èæŠ“å–å®Œæˆï¼ï¼ˆYahoo å€‹è‚¡æ–°è + é‰…äº¨ï¼‰")
