# -*- coding: utf-8 -*-
"""
å…‰å¯¶ç§‘æ–°èæŠ“å–ï¼ˆYahoo + TechNews å¼·åŒ–ç‰ˆï¼‰
âœ” å¤šé—œéµå­—ï¼šå…‰å¯¶ç§‘ / å…‰å¯¶ / 2301
âœ” Yahoo æŠ“å¤šé  + æ–°èˆŠç‰ˆæ”¯æ´
âœ” TechNews å¤šé è§£æ
âœ” æŠ“æ–°èå…¨æ–‡
âœ” æ™‚é–“éæ¿¾ï¼š36 å°æ™‚å…§
âœ” å¯«å…¥ Firestoreï¼ˆä¸å­˜è‚¡åƒ¹ï¼‰
"""

import os
import time
import requests
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore

# -----------------------------
# Firestore åˆå§‹åŒ–
# -----------------------------
if not firebase_admin._apps:
    cred = credentials.Certificate(os.environ["GOOGLE_APPLICATION_CREDENTIALS"])
    firebase_admin.initialize_app(cred)

db = firestore.client()
COLL_NAME = "NEWS_LiteOn"
KEYWORDS = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]  # å¤šé—œéµå­—
MAX_HOURS = 36
HEADERS = {"User-Agent": "Mozilla/5.0"}

# -----------------------------
# æ™‚é–“éæ¿¾
# -----------------------------
def is_recent(dt):
    return (datetime.now(timezone.utc) - dt).total_seconds() <= MAX_HOURS * 3600


# -----------------------------
# Yahoo å¼·åŒ–æŠ“å–
# -----------------------------
def fetch_yahoo(limit_each=30):
    print("\nğŸ“¡ Yahoo å¼·åŒ–æŠ“å–ä¸­...")

    base = "https://tw.news.yahoo.com"
    all_news, seen_links = [], set()

    for keyword in KEYWORDS:
        print(f"\nğŸ” Yahoo æœå°‹é—œéµå­—ï¼š{keyword}")

        for page in range(1, 4):  # æŠ“ 3 é 
            url = f"{base}/search?p={keyword}&sort=time&b={(page-1)*10+1}"

            try:
                r = requests.get(url, headers=HEADERS, timeout=10)
                soup = BeautifulSoup(r.text, "html.parser")

                # æ–°ç‰ˆ Yahoo
                links = soup.select("a.js-content-viewer")

                # èˆŠç‰ˆ Yahoo
                if not links:
                    links = soup.select("h3 a")

                for a in links:
                    href = a.get("href")
                    if not href:
                        continue

                    if not href.startswith("http"):
                        href = base + href

                    if href in seen_links:
                        continue
                    seen_links.add(href)

                    # æŠ“å…¨æ–‡
                    try:
                        r2 = requests.get(href, headers=HEADERS, timeout=10)
                        s2 = BeautifulSoup(r2.text, "html.parser")

                        title = s2.find("h1")
                        if not title:
                            continue
                        title = title.get_text(strip=True)

                        paras = s2.select("article p") or s2.select("p")
                        content = "\n".join(
                            p.get_text(strip=True)
                            for p in paras
                            if len(p.get_text(strip=True)) > 40
                        )[:1500]

                        # æ™‚é–“
                        time_tag = s2.find("time")
                        if not time_tag or not time_tag.has_attr("datetime"):
                            continue

                        published_dt = datetime.fromisoformat(
                            time_tag["datetime"].replace("Z", "+00:00")
                        )
                        if not is_recent(published_dt):
                            continue

                        all_news.append({
                            "title": title,
                            "content": content,
                            "time": published_dt.strftime("%Y-%m-%d %H:%M"),
                            "source": "Yahoo"
                        })

                        time.sleep(0.3)

                    except:
                        continue

            except:
                continue

    return all_news


# -----------------------------
# TechNews å¼·åŒ–æŠ“å–
# -----------------------------
def fetch_technews(limit_pages=3):
    print("\nğŸ“¡ TechNews å¼·åŒ–æŠ“å–ä¸­...")

    base = "https://technews.tw"
    all_news, seen_links = [], set()

    for keyword in KEYWORDS:
        print(f"\nğŸ” TechNews æœå°‹é—œéµå­—ï¼š{keyword}")
        url = f"https://technews.tw/google-search/?googlekeyword={keyword}"

        try:
            r = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")

            raw_links = soup.find_all("a", href=True)
            links = []

            for a in raw_links:
                href = a["href"]
                if href.startswith(base) and "/tag/" not in href:
                    if href not in links:
                        links.append(href)

            links = links[:50]  # é¿å…æŠ“å¤ªå¤š

        except:
            continue

        # æŠ“æ¯ç¯‡æ–‡ç« 
        for link in links:
            if link in seen_links:
                continue
            seen_links.add(link)

            try:
                r2 = requests.get(link, headers=HEADERS, timeout=10)
                s2 = BeautifulSoup(r2.text, "html.parser")

                title_tag = s2.find("h1")
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)

                time_tag = s2.find("time", class_="entry-date")
                if not time_tag:
                    continue

                published_dt = datetime.strptime(
                    time_tag.get_text(strip=True), "%Y/%m/%d %H:%M"
                ).replace(tzinfo=timezone.utc)

                if not is_recent(published_dt):
                    continue

                paras = s2.select("article p") or s2.select("p")
                content = "\n".join(
                    p.get_text(strip=True)
                    for p in paras
                    if len(p.get_text(strip=True)) > 40
                )[:1500]

                all_news.append({
                    "title": title,
                    "content": content,
                    "time": published_dt.strftime("%Y-%m-%d %H:%M"),
                    "source": "TechNews"
                })

                time.sleep(0.3)

            except:
                continue

    return all_news


# -----------------------------
# Firestore å¯«å…¥
# -----------------------------
def save_news(news_list):
    if not news_list:
        print("âš ï¸ æ²’æœ‰æ–°èå¯å¯«å…¥")
        return

    today = datetime.now().strftime("%Y%m%d")
    ref = db.collection(COLL_NAME).document(today)

    data = {}
    for i, n in enumerate(news_list, 1):
        data[f"news_{i}"] = n

    ref.set(data)
    print(f"ğŸ”¥ Firestore å·²å¯«å…¥ â†’ {COLL_NAME}/{today}")
    print(f"ğŸ“¦ å…± {len(news_list)} å‰‡æ–°è")


# -----------------------------
# ä¸»ç¨‹å¼
# -----------------------------
if __name__ == "__main__":
    all_news = []

    # Yahoo + TechNews
    all_news += fetch_yahoo()
    all_news += fetch_technews()

    save_news(all_news)

    print("\nğŸ‰ Yahoo + TechNews æ–°èæŠ“å–å®Œæˆï¼")
