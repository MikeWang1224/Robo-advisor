# -*- coding: utf-8 -*-
"""
å…‰å¯¶ç§‘æ–°èæŠ“å– + Groq æƒ…ç·’åˆ†æï¼ˆYahoo + é‰…äº¨ç¶²ï¼‰
âœ” 3 å¤©å…§
âœ” ä¸­æ–‡é‡‘èæ¨¡å‹è©•ä¼°ï¼šåˆ©å¤š/åˆ©ç©º/ä¸­æ€§ + -1~1 åˆ†æ•¸ + äº‹ä»¶é¡å‹
âœ” å¯«å…¥ Firestoreï¼ˆNEWS_LiteOnï¼‰
"""

import os
import requests 
import json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
import firebase_admin
from firebase_admin import credentials, firestore
from groq import Groq  # <--- æ–°å¢

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

HEADERS = {'User-Agent': 'Mozilla/5.0'}
client = Groq(api_key=os.environ.get("GROQ_API_KEY"))   # <--- Groq åˆå§‹åŒ–

# Firestore åˆå§‹åŒ–
cred = credentials.Certificate(os.environ["GOOGLE_APPLICATION_CREDENTIALS"])
firebase_admin.initialize_app(cred)
db = firestore.client()


# ----- æ™‚é–“éæ¿¾ï¼ˆ72 å°æ™‚ï¼‰ -----
def is_recent(published_time, hours=72):
    now = datetime.now().astimezone()
    return (now - published_time) <= timedelta(hours=hours)


# ----- æŠ“æ–‡ç« å…§å®¹ -----
def fetch_article_content(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')

        paragraphs = soup.select('article p') or soup.select('p')
        text = "\n".join(p.get_text(strip=True) for p in paragraphs)
        return text[:1500] + ('...' if len(text) > 1500 else '')
    except:
        return ""


# ----- é—œéµå­—åˆ¤æ–· -----
def contains_keyword(title, content):
    keywords = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
    text = (title + " " + content)
    return any(k in text for k in keywords)


# ----- Groq AIï¼šæ–°èæƒ…ç·’ã€äº‹ä»¶åˆ†æ -----
def analyze_news_groq(title, content):
    try:
        prompt = f"""
ä½ æ˜¯ä¸€ä½å°è‚¡é‡‘èåˆ†æå¸«ï¼Œè«‹é–±è®€ä»¥ä¸‹æ–°èï¼Œé‡å°ã€Œå…‰å¯¶ç§‘(2301)ã€è©•ä¼°å°è‚¡åƒ¹çš„å¯èƒ½å½±éŸ¿ã€‚

è«‹è¼¸å‡º JSON æ ¼å¼ï¼š
{{
  "sentiment": "åˆ©å¤š or åˆ©ç©º or ä¸­æ€§",
  "score": -1.0 ~ 1.0,
  "reason": "ç°¡çŸ­åŸå› ï¼ˆå¿…å¡«ï¼‰",
  "event": "æ¥å–®/è²¡å ±/æ³•èªª/åœå·¥/å‡ºè²¨/è¨´è¨Ÿ/æ–°å“/ä¸€èˆ¬æ–°è"
}}

æ–°èæ¨™é¡Œï¼š{title}
æ–°èå…§å®¹ï¼š{content}
"""

        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="qwen-2.5-32b",  # ä¸­æ–‡æ¨¡å‹ï¼ˆé‡‘èé©ç”¨ï¼‰
            temperature=0.3,
            max_tokens=300
        )

        text = response.choices[0].message.content
        try:
            result = json.loads(text)
        except:
            return None

        return result

    except Exception as e:
        print("âŒ GROQ åˆ†æéŒ¯èª¤ï¼š", e)
        return None


# =============================
#  Yahoo æ–°è
# =============================
def fetch_yahoo_news(limit=80, pages=4):
    print("ğŸ“¡ æŠ“å– Yahoo æ–°è")
    base = "https://tw.news.yahoo.com"
    
    news_list = []
    seen = set()

    for page in range(1, pages + 1):
        url = f"https://tw.news.search.yahoo.com/search?p=å…‰å¯¶ç§‘&b={(page-1)*10+1}"
        r = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(r.text, 'html.parser')

        candidates = (
            soup.select("a.js-content-viewer") +
            soup.select("h3 a") +
            soup.select("a.d-ib") +
            soup.select("a[data-ylk]")
        )

        for a in candidates:
            if len(news_list) >= limit:
                return news_list

            title = a.get_text(strip=True)
            if not title or title in seen:
                continue
            seen.add(title)

            href = a.get("href")
            if not href:
                continue
            if href.startswith("/"):
                href = base + href

            content = fetch_article_content(href)
            if not contains_keyword(title, content):
                continue

            try:
                r2 = requests.get(href, headers=HEADERS)
                s2 = BeautifulSoup(r2.text, 'html.parser')
                time_tag = s2.find("time")

                if not time_tag or not time_tag.has_attr("datetime"):
                    continue

                published_dt = datetime.fromisoformat(
                    time_tag["datetime"].replace("Z", "+00:00")
                ).astimezone()

                if not is_recent(published_dt):
                    continue

            except:
                continue

            # *** AI åˆ†æ ***
            ai = analyze_news_groq(title, content)

            news_list.append({
                "title": title,
                "content": content,
                "published_time": published_dt,
                "source": "Yahoo",
                "ai": ai
            })

    return news_list


# =============================
#  é‰…äº¨ç¶²
# =============================
def fetch_cnyes_news(limit=40):
    print("ğŸ“¡ æŠ“å– é‰…äº¨ç¶²")

    keywords = ["å…‰å¯¶ç§‘", "å…‰å¯¶", "2301"]
    news_list = []
    seen = set()

    for kw in keywords:
        try:
            url = f"https://api.cnyes.com/media/api/v1/search/list?keyword={kw}&limit=50"
            r = requests.get(url, headers=HEADERS, timeout=10)
            items = r.json().get("items", {}).get("data", [])

            for item in items:
                if len(news_list) >= limit:
                    return news_list

                title = item.get("title", "")
                if not title or title in seen:
                    continue
                seen.add(title)

                timestamp = item.get("publishAt", 0)
                if not timestamp:
                    continue

                published_dt = datetime.fromtimestamp(timestamp).astimezone()
                if not is_recent(published_dt):
                    continue

                article_url = f"https://news.cnyes.com/news/id/{item.get('newsId')}?exp=a"
                content = fetch_article_content(article_url)

                if not contains_keyword(title, content):
                    continue

                # *** AI åˆ†æ ***
                ai = analyze_news_groq(title, content)

                news_list.append({
                    "title": title,
                    "content": content,
                    "published_time": published_dt,
                    "source": "é‰…äº¨ç¶²",
                    "ai": ai
                })

        except Exception as e:
            print("é‰…äº¨ç¶²æŠ“å–éŒ¯èª¤ï¼š", e)

    return news_list


# =============================
# Firestore å„²å­˜
# =============================
def save_news(news_list):
    doc_id = datetime.now().strftime("%Y%m%d")
    ref = db.collection("NEWS_LiteOn").document(doc_id)

    data = {}
    for i, n in enumerate(news_list, 1):
        data[f"news_{i}"] = {
            "title": n["title"],
            "content": n["content"],
            "source": n["source"],
            "published_time": n["published_time"].strftime("%Y-%m-%d %H:%M"),
            "sentiment": n["ai"].get("sentiment") if n.get("ai") else None,
            "score": n["ai"].get("score") if n.get("ai") else None,
            "event": n["ai"].get("event") if n.get("ai") else None,
            "reason": n["ai"].get("reason") if n.get("ai") else None
        }

    ref.set(data)
    print(f"âœ… å·²å­˜å…¥ Firestoreï¼šNEWS_LiteOn/{doc_id}")


# =============================
# ä¸»ç¨‹å¼
# =============================
if __name__ == "__main__":
    yahoo_news = fetch_yahoo_news()
    cnyes_news = fetch_cnyes_news()

    all_news = yahoo_news + cnyes_news

    print(f"ğŸ” å…±æŠ“åˆ° {len(all_news)} å‰‡å…‰å¯¶ç§‘ç›¸é—œæ–°èï¼ˆ3 å¤©å…§ï¼‰")

    if all_news:
        save_news(all_news)

    print("ğŸ‰ å…‰å¯¶ç§‘æ–°èæŠ“å– + Groq åˆ†æå®Œæˆï¼")
